\begin{abstract}
Real time and cost-effective analytics for “Big Data” is now a key ingredient for success in many businesses, scientific research and engineering disciplines, and government endeavors. The Spark platform—which consists of an extensible MapReduce execution engine, pluggable distributed storage engines, and a range of procedural to declarative interfaces—is a mainstream choice for big data analytics. Most practitioners of big data analytics—like computational scientists, systems researchers, and business analysts—lack the expertise to configure the system setting to get good performance. However, it is challenging to automatically identify the best configuration for a diversity of applications and cloud configurations. Unfortunately, Spark’s performance out of the box leaves much to be desired, leading to suboptimal use of resources, time, and money (in pay as-you-go clouds). We introduce a new approach of adaptively the spark configurations for big data analytics. It builds on Spark while adapting to user needs and system workloads to provide good performance automatically, without any need for users to understand and manipulate the many tuning knobs in Spark.

\end{abstract}
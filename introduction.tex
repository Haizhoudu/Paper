\section{introduction}
\par With the rapid and continuous development of ``Big Data" analyze like distributed natural language processing \cite{TT2007Largelanguage}, deep learning for image recognition \cite{dean2012Large}, genome analysis \cite{L2010cloudcomputing}, astronomy \cite{Berriman2010application} and particle accelerator data processing \cite{bird2011}. These applications differ from traditional analytics workloads (e.g., SQL queries) in that they are not only data-intensive but also computation-intensive, and typically run for a long time (and hence are expensive). Along with new workloads, we have seen widespread adoption of spark platform with large data sets being hosted \cite{commoncrawlurl}, and the emergence of sophisticated analytics services, such as machine learning, being offered by spark providers\cite{Machinelearningurl}.

\par With the era of big data coming, big data technologies have attracted more and more attention from the academia, industry and governments in recent years. The traditional computing model in the era of big data cannot meet the demands of performance and efficiency, Hadoop \cite{apachehadoop}, Spark \cite{apachespark}, Storm \cite{apachestorm} and other distributed frameworks emerge as time goes on. Among several big data frameworks, Apache Spark has become the most popular and universal big data processing platform because of its excellent performance and rich application scenarios support.

\par With the wide application of Spark, it also exposes some problems in practical applications. One of the most important aspects is the performance problem. Due to unreasonable parameter configuration of Spark, the performance of the Spark applications is difficult to achieve theoretical peak speed of computer, so how to optimize the performance of Spark applications is one problem that is worthy of research.

\par Apache Spark has more than 180 configuration parameters, which can be adjusted by users according to their own specific application so as to optimize the performance \cite{apachespark},. This is the most simple and effective way to optimize the performance of Spark applications. On the one hand, the large parameter space enables a lot of opportunities for significant performance improvement by careful parameter tuning, but on the other hand, the large parameter space and the complex interactions among the parameters make the appropriate parameter tuning a challenging task.

\par Currently, there are two ways to tuning the configuration parameters of big data platforms. Firstly, these parameters are tuned manually by trial and error, which is ineffective and time consuming due to the large parameter space and the complex interactions among the parameters. In order to overcome the disadvantages of tuning parameters manually by trial and error, some researchers proposed a cost-based
(analytical) performance modeling method to tuning the parameters of Hadoop platform \cite{apachehadoop}. However, Spark is much different from Hadoop in the underlying implementation mechanism, so that we can’t use this method on Apache Spark directly. At the same time, it suffers from several drawbacks when modeling Spark platform based on cost. First, cost-based modeling is a white box approach where a deep knowledge
about a system’s internals is required, and since the system that comprises the software stack (operating system, JVM, Spark, and workloads) and the hardware stack is very complex, it becomes very difficult to capture this complexity with the cost-based model. Second, Spark has several customizable components where users can plug in their own policies, such as scheduling and shuffling policies, which makes it necessary for the performance model to support these different policies.

\par These drawbacks motivate us to explore new approach based performance models, which are black box models. Compared with the existing approaches, black box models have two main advantages. First, the recommendations of an auto-tuner using these kinds of models are based on observations of the actual system performance under a particular workload and cluster. Second, they are usually simpler to build than white box models as there is no need for detailed information about the system’s internals. Finally, our
proposed method relies on training data to learn the performance model, which makes it more robust and flexible.

\par The rest of the paper is organized as follows. The related work is discussed in Section \ref{sec:related}. A brief overview of Apache Spark and some definition are presented in Section \ref{sec:definition}. A performance model for multiple model fusion based on machine learning and optimization method of spark platform will be proposed in Section \ref{sec:approach}. Experimental methodology and experimental result analysis are given in Section \ref{sec:evaluation}. Finally, the conclusions and future work are summarized in Section \ref{sec:conclusion}. 

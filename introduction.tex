\section{introduction}
\par With the rapid and continuous development of big data analyze like distributed natural language processing \cite{brants2007large}, deep learning for image recognition \cite{dean2012Large}, genome analysis \cite{L2010cloudcomputing}, astronomy \cite{Berriman2010application} and particle accelerator data processing \cite{bird2011}. These applications differ from traditional analytics workloads (e.g., SQL queries) in that they are not only data-intensive but also computation-intensive, and typically run for a long time (and hence are expensive). Along with new workloads, we have seen widespread adoption of efficient big data anaytics platform with large datasets being hosted \cite{commoncrawlurl}, and the emergence of sophisticated analytics services, such as machine learning, being offered by data anaytics providers\cite{Machinelearningurl}.

\par According to IDC(Internet Data Center) predictions by 2020, the size of the digital universe will exceed 44ZB \cite{IDC2012}. So that we have entering the era of big data, and big data analytics technologies have attracted more and more attention from the academia, industry and governments in recent years. The traditional computing model in the era of big data cannot meet the demands of performance and efficiency, Hadoop \cite{apachehadoop}, Spark \cite{apachespark}, Storm \cite{apachestorm} and other distributed frameworks emerge as time goes on. Among several big data frameworks, Apache Spark has become the most popular and universal big data processing platform because of its excellent performance and rich application scenarios support.

\par With the wide application of Spark, it also exposes some problems in practical applications. One of the most important aspects is the performance problem. Due to unreasonable parameter configuration of Spark, the performance of Spark applications is difficult to achieve theoretical peak speed of computer, so how to optimize the performance of Spark applications is one problem that is worthy of research.

\par Apache Spark has more than 180 configuration parameters, which can be adjusted by users according to their own specific application so as to optimize the performance \cite{apachespark}. This is the most simple and effective way to optimize the performance of Spark applications. On the one hand, a lots of parameters enable a lot of opportunities for significant performance improvement by careful parameter tuning, but on the other hand, the large parameter space and the complex interactions among the parameters make the appropriate parameter tuning a challenging task.

\par New practitoners of big data analytics like computational scientists and systems researchers lack the experitse to tune Spark to get good performance. Currently, there are two ways to tuning the configuration parameters of Spark platforms. Firstly, these parameters are tuned manually by trial and error, which is ineffective and time consuming due to a lots of parameters and the complex interactions among the parameters. In order to overcome the disadvantages of tuning parameters manually, some researchers proposed a cost-based (analytical) performance modeling method to tuning the parameters of Hadoop platform \cite{apachehadoop}. However, Spark is much different from Hadoop in the underlying implementation mechanism, so that we can not use this method on Spark directly. Meanwhile, it suffers from several weakness when modeling Spark platform based on cost. First, cost-based modeling is a white box approach where a deep and wide knowledge about a system’s internals is required, and since the system that comprises the software stack (operating system, JVM, Spark, and workloads) and the hardware stack is very complex, it becomes very hard to capture this complexity with the cost-based model. Secondly, Spark has several customizable components where users can plug in their own policies, such as scheduling and encryption policies, which makes it necessary for the performance model to support these different policies.

\par These drawbacks motivate us to explore new approach based performance models, which are black box models. Compared with the existing approaches, black box models have two main advantages. First, the recommendations of an Automatic tuning using these models are based on observations of the actual system performance with logging information. Second, they are usually more simple to build as there is no need for detailed information about the system’s internals. Therefore, our proposed method relies on optimization theory to build the performance model, which makes it more robust and flexible.

\par The rest of the paper is organized as follows. The related work is discussed in Section \ref{sec:related}. A brief overview of Apache Spark and some definition are presented in Section \ref{sec:definition}. A performance model for multiple model fusion based on machine learning and optimization method of spark platform will be proposed in Section \ref{sec:approach}. Experimental methodology and experimental result analysis are given in Section \ref{sec:evaluation}. Finally, the conclusions and future work are summarized in Section \ref{sec:conclusion}. As a appendix, All of our selected parameters which are most important to the optimal configuration of Spark are listed in the Section \ref{sec:appendix} .
